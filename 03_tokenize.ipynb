{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c01c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from typing import Dict, List, Tuple, NamedTuple\n",
    "from transformers import BertTokenizerFast\n",
    "import scml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590fba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length = 32\n",
    "stride = 0\n",
    "add_special_tokens = False\n",
    "return_overflowing_tokens = False\n",
    "return_offsets_mapping = False\n",
    "return_special_tokens_mask = False\n",
    "return_token_type_ids = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc7612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = scml.Timer()\n",
    "tim.start()\n",
    "percentiles=[.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6958ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12899779 entries, 0 to 12899778\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   session  int32 \n",
      " 1   seq      object\n",
      " 2   length   int16 \n",
      "dtypes: int16(1), int32(1), object(1)\n",
      "memory usage: 172.2+ MB\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\"input/sequences.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e25acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=500009, model_max_len=32, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<s>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>', 'additional_special_tokens': ['<click>', '<cart>', '<order>']})\n",
      "model_input_names=['input_ids', 'token_type_ids', 'attention_mask']\n",
      "<unk>=3\n",
      "<pad>=1\n",
      "<s>=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file=\"input/vocab.txt\", \n",
    "    unk_token=\"<unk>\",\n",
    "    sep_token=\"<s>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    additional_special_tokens=[\"click_token\", \"cart_token\", \"order_token\"],\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<click>\", \"<cart>\", \"<order>\"],\n",
    "})\n",
    "unk_token = tokenizer.unk_token\n",
    "unk_id = tokenizer.unk_token_id\n",
    "pad_token = tokenizer.pad_token\n",
    "pad_id = tokenizer.pad_token_id\n",
    "sep_token = tokenizer.sep_token\n",
    "sep_id = tokenizer.sep_token_id\n",
    "print(f\"{repr(tokenizer)}\\nmodel_input_names={tokenizer.model_input_names}\")\n",
    "print(f\"{unk_token}={unk_id}\\n{pad_token}={pad_id}\\n{sep_token}={sep_id}\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0ba608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12899779it [01:31, 140923.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(s1)=18,751,938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sids, s1, s2 = [], [], []\n",
    "# encoder or decoder must have min 2 tokens \n",
    "minlen = 2\n",
    "for t in tqdm(df.itertuples()):\n",
    "    length = int(getattr(t, \"length\"))\n",
    "    if length<2*minlen:\n",
    "        continue\n",
    "    sid = int(getattr(t, \"session\"))\n",
    "    seq = getattr(t, \"seq\").split()\n",
    "    al, bl = [], []\n",
    "    length = model_max_length  # minus 2 for CLS, EOS tokens\n",
    "    i = 0\n",
    "    j = i+length\n",
    "    while j+length<=len(seq):\n",
    "        al.append(seq[i:j])\n",
    "        bl.append(seq[j:j+length])\n",
    "        i += length\n",
    "        j += length\n",
    "    if i<len(seq) and len(seq)-i>=2*minlen:\n",
    "        j = i+((len(seq)-i)//2)\n",
    "        if j%2==1:\n",
    "            j+=1\n",
    "        al.append(seq[i:j])\n",
    "        bl.append(seq[j:])\n",
    "    for i in range(len(al)):\n",
    "        a, b = al[i], bl[i]\n",
    "        if len(a)%2==1:\n",
    "            raise ValueError(\"a must have even length\")\n",
    "        if len(b)%2==1:\n",
    "            raise ValueError(f\"b must have even length. b={b}\")\n",
    "        if len(a)<minlen:\n",
    "            raise ValueError(\"length of a must not be less than minlen\")\n",
    "        if len(b)<minlen:\n",
    "            raise ValueError(\"length of b must not be less than minlen\")\n",
    "        s1.append(\" \".join(a))\n",
    "        s2.append(\" \".join(b))\n",
    "        sids.append(sid)\n",
    "print(f\"len(s1)={len(s1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb12853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"output/sids.json\", \"w\") as f:\n",
    "    json.dump(sids, f)\n",
    "del df, sids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682cbd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "Wall time: 30min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = tokenizer(\n",
    "    s1,\n",
    "    truncation=True, \n",
    "    padding=\"max_length\",\n",
    "    stride=stride,\n",
    "    add_special_tokens=add_special_tokens,\n",
    "    return_overflowing_tokens=return_overflowing_tokens,\n",
    "    return_offsets_mapping=return_offsets_mapping,\n",
    "    return_special_tokens_mask=return_special_tokens_mask,\n",
    "    return_token_type_ids=return_token_type_ids,\n",
    ")\n",
    "print(f\"{repr(x.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e288b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK 3.31%\t19,850,402 out of 600,062,016 tokens\n",
      "PAD 47.34%\t284,042,138 out of 600,062,016 tokens\n",
      "SEP 0.00%\t0 out of 600,062,016 tokens\n",
      "input_ids.shape=(18751938, 32)\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = np.array(x[\"input_ids\"], dtype=np.uint32)\n",
    "n_pad = (input_ids == tokenizer.pad_token_id).sum()\n",
    "n_unk = (input_ids == tokenizer.unk_token_id).sum()\n",
    "n_sep = (input_ids == tokenizer.sep_token_id).sum()\n",
    "d = input_ids.shape[0] * input_ids.shape[1]\n",
    "print(f\"UNK {n_unk/d*100:.2f}%\\t{n_unk:,} out of {d:,} tokens\")\n",
    "print(f\"PAD {n_pad/d*100:.2f}%\\t{n_pad:,} out of {d:,} tokens\")\n",
    "print(f\"SEP {n_sep/d*100:.2f}%\\t{n_sep:,} out of {d:,} tokens\")\n",
    "print(f\"input_ids.shape={input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4829c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"output/x.json\", \"w\") as f:\n",
    "    json.dump(dict(x), f)\n",
    "del x, s1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6fc93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "Wall time: 29min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = tokenizer(\n",
    "    s2,\n",
    "    truncation=True, \n",
    "    padding=\"max_length\",\n",
    "    stride=stride,\n",
    "    add_special_tokens=add_special_tokens,\n",
    "    return_overflowing_tokens=return_overflowing_tokens,\n",
    "    return_offsets_mapping=return_offsets_mapping,\n",
    "    return_special_tokens_mask=return_special_tokens_mask,\n",
    "    return_token_type_ids=return_token_type_ids,\n",
    ")\n",
    "print(f\"{repr(y.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d2da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"output/y.json\", \"w\") as f:\n",
    "    json.dump(dict(y), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351124eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken 2:05:03.371182\n"
     ]
    }
   ],
   "source": [
    "tim.stop()\n",
    "print(f\"Total time taken {str(tim.elapsed)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
